```
```{r,message=FALSE, error=FALSE}
if(!require(stringr)) install.packages("stringr")
library(stringr)
```
```{r,message=FALSE, error=FALSE}
if(!require(stringr)) install.packages("stringr")
library(stringr)
```
x <- "https://raw.githubusercontent.com/jwindokun/FinalProject/master/data/gourmetFoods_test.txt"
try ({
line=readLines(x, n = -1L, ok = TRUE, warn = FALSE)
})
```
```{r}
df <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)
t = character()
n = 1
d <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)
#for (i in 1:length(line)){
# for the purpose of demonstration will only read in the first 10000 lines of the file
for (i in 1:1000){
try({
w = str_split_fixed(line[i], ":", 2)
if (!is.na(w[1])){
if (n < 11) {
d[1, n] = str_trim(w[2])
n = n + 1
} else {
df = rbind(df, d)
n = 1
}
}
}, silent = TRUE)
}
str(df)
#clean the data
df$time = as.POSIXct(as.numeric(df$time), origin="1970-01-01")
df$price[df$price == "unknown"] = 0
df$price = as.numeric(df$price)
df$score = as.numeric(df$score)
for (i in 1 :length(df$helpfulness)){
e = df$helpfulness[i]
e = strsplit(e, split = "/", fixed = TRUE)[[1]]
e = as.numeric(e[1])/as.numeric(e[2])
if (is.finite(e)) {
df$helpfulness[i] = e
} else {
df$helpfulness[i] = 0
}
}
df$helpfulness = as.numeric(df$helpfulness)
```
Print out the dowloaded, transformed, cleaned data
```{r}
str(df)
df[1,]
```
Print out the dowloaded, transformed, cleaned data
```{r}
str(df)
df[1,]
```
Print out the dowloaded, transformed, cleaned data
```{r}
str(df)
df[1,]
```
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
---
title: "607_Final_Project"
author: "Adejare Windokun"
date: "Friday, December 12, 2014"
output: html_document
---
# **bold** Project Proposal Data Mash ups in R
# **bold** Proposal describes your motivation for performing this analysis.
Data Mashups whereby data is obtained from different sources (heterogeneous data) and merged together to provide to a more comprehensive view of the data is of huge advantage especially with the ease of accessing data over the internet, the ability to query and use third party tools including APIs and the advent of huge datasets made available by both the government and private entities.
During the semester, while I have worked on single datasets, I haven’t had the opportunity to work on multiple diverse datasets, with all the complexities that this entails – varied data formats, incomplete data, need for data transformations, and especially the need to work with third party APIs. This project will provide me the opportunity to do all these.
# **bold** Your project has a recognizable “data science workflow.
My datasets consists of:
1.  A subset of the Amazon Reviews which is hosted at http://snap.stanford.edu/data/web-Amazon.html, specifically the dataset “Gourmet_Foods”. This dataset is formatted as such:
product/productId: asin, e.g. amazon.com/dp/B00006HAXW
product/title: title of the product
product/price: price of the product
review/userId: id of the user, e.g. A1RSDE90N6RSZF
review/profileName: name of the user
review/helpfulness: fraction of users who found the review helpful
review/score: rating of the product
review/time: time of the review (unix time)
review/summary: review summary
review/text: text of the review
where by a single row of data (item) is spread across 10 lines with a space in between subsequent items. Data transformation which has to be performed include extracting the data, limiting the dataset to manageable number, transforming the data into a table structure, cleaning the data and removing the unwanted labels, transforming columns in the dataset (covert the time column from character to POSIXct() which R understands.
2.	Reviewer data from Amazon.com using the R packages rvest and XML, which is returned in HTML format which has to be parsed to extract the relevant data which further has to be processed and converted to the required data type suitable for use in R.
3.	Sentiment Analysis using AlchemyAPI (AlchemyAPI.com). Sentiment analysis was performed on both the summary and text fields which are reviews that the reviewer made on each item. This data has to be sent to AlchemyAPI for analysis using HTML, and the result which is returned in the JSON format has to be parsed to extract the relevant data, followed by data transformation to the required data type in R for further processing.
# **bold**  Project includes data from at least two different types of data sources (e.g., two or more of these: relational, CSV, Neo4J, web page, MongoDB, etc.)
My project includes (a) data from a flat file (Amazon Reviews from snap.stanford.edu), (b) live from webpages (amazon.com - http://www.amazon.com/gp/cdp/member-reviews/userID) and (c) using an API from AlchemyAPI.com
Project includes at least one data transformation operation. [Examples: transforming from wide to long; converting columns to date format]
Each of the data sources required extensive data transformation before it could be used in R
Project includes at least one statistical analysis and at least one graphics that describes or validates your data.
Project includes at least one graphic that supports your conclusion(s).
# **bold**  Project includes at least one statistical analysis that supports your conclusion(s).
My project looks at the correlation between "Helpfullness" and "Reviewer Scores" for items and the correlation between Sentiment Analysis on "Summary Text" and
"Reviewes"
# **bold**  Project includes at least one feature that we did not cover in class! There are many examples:
Features not covered in class:
1.	AlchemyAPI (http://www.alchemyapi.com/)
2.	R packages
a.	Stringr
b.	Rvest
c.	RJSON
d.	Sqldf
e.	Rcurl
# **bold**  Presentation. Did you show (at least) one challenge you encountered in code and/or data, and what you did when you encountered that challenge? If you didn’t encounter any challenges, your assignment was clearly too easy for you!
There were multiple unexpected challenges that occurred during this project.
1.	Obtaining the data. While the Amazon Review dataset is supposed to be public at http://snap.stanford.edu, one has to request permission from Stanford and this caused a delay.
2.	AlchemyAPI provides APIs to use their services, and even instructions on how these APIs work. However, they do not provide one for R, and therefore I was left to use the interface that they provide through REST. This unfortunately, was not well documented and required the manipulation of the URL, and much more manipulation of the return dataset which was in JSON format. The use of the API, for example makes submitting and extracting data a simple two line process, which however in R using the REST interface took multiple lines of code and the use of multiple R packages.
3.	Limits on data extraction using the web interface: Querying Amazon.com for reviewers other reviews was a slow process due to the fact that the query had to be sent to amazon.com using the “post” service in HTML, and the return in HTLM had to be parsed to obtain the necessary information. Also, sending the requests too fact resulted in a server denial by the amazon servers -  hence I had to build in a pause between each request.
4.	Limits on allowable use AlchemyAPI. AlchemyAPI only allows 1000 request per day from a free account . Each of my rows of data required two requests, and therefore I was limited to only being able to obtain data on 500 rows per day. In addition, too frequent requests led to a denial of service, hence I had to insert a pause between requests. Additionally, using the  REST interface led to unexpected errors (as compared to using the API) and this had to be taken into account and coded for.
5.	Inconsistent data formats. The amazon.com website does not provide a consistent format and therefore parsing the HTML return data was a slow process and had to account for this inconsistency. This led to much more coding and error handling than should have been necessary.
# **bold** Code
Install required libraries
```{r,message=FALSE, error=FALSE}
if(!require(stringr)) install.packages("stringr")
library(stringr)
```
Code to read local files
fpath = paste(getwd(), "/data/", sep = "")
fileName = "gourmetFoods.txt"
inputFile <- paste(fpath, fileName, sep = "")
conn  <- file(inputFile, open = "r")
line=readLines(conn, n = -1L, ok = TRUE, warn = FALSE)
close(conn)
Will read in the data from github
```{r}
x <- "https://raw.githubusercontent.com/jwindokun/FinalProject/master/data/gourmetFoods_test.txt"
try ({
line=readLines(x, n = -1L, ok = TRUE, warn = FALSE)
})
```
```{r}
try({
df <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)
n = 1
d <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)
}, silent = TRUE)
#for (i in 1:length(line)){
# for the purpose of demonstration will only read in the first 10000 lines of the file
for (i in 1:1000){
try({
w = str_split_fixed(line[i], ":", 2)
if (!is.na(w[1])){
if (n < 11) {
d[1, n] = str_trim(w[2])
n = n + 1
} else {
df = rbind(df, d)
n = 1
}
}
}, silent = TRUE)
}
str(df)
#clean the data
df$time = as.POSIXct(as.numeric(df$time), origin="1970-01-01")
df$price[df$price == "unknown"] = 0
df$price = as.numeric(df$price)
df$score = as.numeric(df$score)
for (i in 1 :length(df$helpfulness)){
e = df$helpfulness[i]
e = strsplit(e, split = "/", fixed = TRUE)[[1]]
e = as.numeric(e[1])/as.numeric(e[2])
if (is.finite(e)) {
df$helpfulness[i] = e
} else {
df$helpfulness[i] = 0
}
}
df$helpfulness = as.numeric(df$helpfulness)
```
Print out the dowloaded, transformed, cleaned data
```{r}
str(df)
df[1,]
```
```{r, message=FALSE, error=FALSE}
if(!require(stringr)) install.packages("stringr")
library(stringr)
```
```{r, message=FALSE, error=FALSE,warning=FALSE}
if(!require(stringr)) install.packages("stringr")
library(stringr)
```
```{r, message=FALSE, error=FALSE,warning=FALSE}
```
---
title: "607_Final_Project"
author: "Adejare Windokun"
date: "Friday, December 12, 2014"
output: html_document
---
hkhkl
```{r}
if(!require(stringr)) install.packages("stringr")
library(stringr)
```
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
Print out the dowloaded, transformed, cleaned data
```{r}
str(df)
df[1,]
```
Print out the dowloaded, transformed, cleaned data
```{r}
str(df)
df[1,]
``
```{r}
str(df)
df[1,]
```
```{r}
df[1,]
```
```{r}
str(df)
df[1,]
```
```{r}
str(df)
df[1,]
```
clean the data
```{r}
str(df)
df$time = as.POSIXct(as.numeric(df$time), origin="1970-01-01")
df$price[df$price == "unknown"] = 0
df$price = as.numeric(df$price)
df$score = as.numeric(df$score)
for (i in 1 :length(df$helpfulness)){
try({
e = df$helpfulness[i]
e = strsplit(e, split = "/", fixed = TRUE)[[1]]
e = as.numeric(e[1])/as.numeric(e[2])
if (is.finite(e)) {
df$helpfulness[i] = e
} else {
df$helpfulness[i] = 0
}
}, silent = TRUE)
}
df$helpfulness = as.numeric(df$helpfulness)
```
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
}
if(!require(stringr)) install.packages ("stringr")
library(stringr)
# Code to read local files
# fpath = paste(getwd(), "/data/", sep = "")
# fileName = "gourmetFoods.txt"
# inputFile <- paste(fpath, fileName, sep = "")
# conn  <- file(inputFile, open = "r")
# line=readLines(conn, n = -1L, ok = TRUE, warn = FALSE)
# close(conn)
x <- "https://raw.githubusercontent.com/jwindokun/FinalProject/master/data/gourmetFoods_test.txt"
try ({
line=readLines(x, n = -1L, ok = TRUE, warn = FALSE)
})
df <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)
t = character()
n = 1
d <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)
#for (i in 1:length(line)){
# for the purpose of demonstration will only read in the first 10000 lines of the file
for (i in 1:1000){
try({
w = str_split_fixed(line[i], ":", 2)
if (!is.na(w[1])){
if (n < 11) {
d[1, n] = str_trim(w[2])
n = n + 1
} else {
df = rbind(df, d)
n = 1
}
}
}, silent = TRUE)
}
str(df)
#clean the data
df$time = as.POSIXct(as.numeric(df$time), origin="1970-01-01")
df$price[df$price == "unknown"] = 0
df$price = as.numeric(df$price)
df$score = as.numeric(df$score)
for (i in 1 :length(df$helpfulness)){
e = df$helpfulness[i]
e = strsplit(e, split = "/", fixed = TRUE)[[1]]
e = as.numeric(e[1])/as.numeric(e[2])
if (is.finite(e)) {
df$helpfulness[i] = e
} else {
df$helpfulness[i] = 0
}
}
df$helpfulness = as.numeric(df$helpfulness)
str(df)
df[1,]
if(!require(RCurl)) install.packages ("RCurl")
library(RCurl)
if(!require(rjson)) install.packages ("rjson")
library(rjson)
# Code to read local file
# fpath = paste(getwd(), "/data/", sep = "")
# fileName = "gourmetFoods.RData"
# w = load(paste(fpath, fileName, sep = "")
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"
try ({
load(url(githubURL))
})
api_key = "45319ace2f7c43fc55d05c2f973ba6261a5de4f0"
api = paste("http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=", api_key, "&outputMode=json", "&text=", sep = "")
gourmetFoodsAPI = gourmetFoods
gourmetFoodsAPI[1,]
#for i in 1:nrow(gourmetfoodsAPI){
for (i in 1:2){
if (length(gourmetFoodsAPI$text[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$text[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apitextType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apitextScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
if (length(gourmetFoodsAPI$summary[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$summary[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apisummaryType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apisummaryScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
Sys.sleep(2)
}
gourmetFoodsAPI[1,]
source('~/.active-rstudio-document', echo=TRUE)
if(!require(stringr)) install.packages("stringr")
library(stringr)
if(!require(RCurl)) install.packages ("RCurl")
library(RCurl)
if(!require(rjson)) install.packages ("rjson")
library(rjson)
```{r sentimentAnalysis}
# Code to read local file
# fpath = paste(getwd(), "/data/", sep = "")
# fileName = "gourmetFoods.RData"
# w = load(paste(fpath, fileName, sep = "")
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"
try ({
load(url(githubURL))
})
api_key = "45319ace2f7c43fc55d05c2f973ba6261a5de4f0"
api = paste("http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=", api_key, "&outputMode=json", "&text=", sep = "")
gourmetFoodsAPI = gourmetFoods
gourmetFoodsAPI[1,]
#for i in 1:nrow(gourmetfoodsAPI){
for (i in 1:2){
if (length(gourmetFoodsAPI$text[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$text[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apitextType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apitextScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
if (length(gourmetFoodsAPI$summary[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$summary[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apisummaryType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apisummaryScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
Sys.sleep(2)
}
gourmetFoodsAPI[1,]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
```
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"
try ({
load(url(githubURL))
})
t = 0
#Users with the most reviews
sql = "SELECT Count(gourmetFoods.userID) AS numReviews, gourmetFoods.profileName FROM gourmetFoods GROUP BY gourmetFoods.profileName
HAVING (((gourmetFoods.profileName)<>'unknown')) ORDER BY Count(gourmetFoods.userID) DESC limit 10 ;"
result = sqldf(sql)
ggplot(result, aes(x = reorder(profileName, numReviews), y = numReviews)) + geom_bar(stat="identity", fill="lightblue", colour="black") +
labs(title = "Number of Reviews per Reviewer (Top Ten Reviewers)", x = "Reviewer", y = "Count") + theme_bw()+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Sys.sleep (t)
#Average Review Score by Reviewer
sql = "SELECT Count(gourmetFoods.userID) AS numReviews, gourmetFoods.profileName, Avg(gourmetFoods.score) AS AvgOfscore, Max(gourmetFoods.score) AS MaxOfscore, Min(gourmetfoods.score) AS MinOfscore
FROM gourmetFoods GROUP BY gourmetFoods.profileName HAVING (((gourmetFoods.profileName)<>'unknown')) ORDER BY Count(gourmetFoods.userID) DESC limit 10;"
result = sqldf(sql)
ggplot(result, aes(x = reorder(profileName, AvgOfscore), y = AvgOfscore)) + geom_bar(stat="identity", fill="green", colour="red") +
labs(title = "Average Reviewer Score By Reviewer (Top Ten Reviewers)", x = "Reviewer", y = "Average Score") + theme_bw()+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Sys.sleep (t)
#Average helpfulness per reviewer
sql = "SELECT gourmetFoods.userID, gourmetFoods.profileName, Avg(gourmetFoods.helpfulness) AS aveOfhelpfulness, Count(gourmetFoods.helpfulness) AS countOfHelpfulness
FROM gourmetFoods GROUP BY gourmetFoods.userID, gourmetFoods.profileName ORDER BY Avg(gourmetFoods.helpfulness)  DESC , Count(gourmetFoods.helpfulness)
DESC limit 10;"
result = sqldf(sql)
ggplot(result, aes(x = reorder(profileName, aveOfhelpfulness),  y = aveOfhelpfulness,fill = countOfHelpfulness)) + geom_bar(stat="identity", fill = "brown", colour="red") +
labs(title = "Average Helpfullness Score By Reviewer (Top Ten Reviewers)", x = "Reviewer", y = "Average Score") + theme_bw()+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
Sys.sleep (t)
#Ploting Helpfulness score with Reviewer score
ggplot(gourmetFoods, aes(x=gourmetFoods$score,y=gourmetFoods$helpfulness)) + geom_point(position=position_jitter(w=0.1,h=0.0), color ="red") +
geom_smooth(method="lm", se=FALSE, fill = "blue", size = 2) + ylab('Helpfulness Score') + xlab('Reviewer Score') + ggtitle("Scatter Plot of Helpfullness Score vs Reviwer Score")
# Statistics
fit = lm(helpfulness ~score, data = gourmetFoods)
coef(fit)
anova(fit)
summary(fit)
Sys.sleep (t)
# look at Sentiment Analysis of summary and text
# fpath = paste(getwd(),"/data/gourmetFoodsAPI.RData", sep = "" )
# w = load(fpath)
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoodsAPI.RData"
try ({
load(url(githubURL))
})
ggplot(gourmetFoodsAPI, aes(x=gourmetFoodsAPI$apisummaryScore,y=gourmetFoodsAPI$apitextScore)) + geom_point(position=position_jitter(w=0.1,h=0.0), color ="red") +
geom_smooth(method="lm", se=FALSE, fill = "blue", size = 2) + ylab('Text Score') + xlab('Summary Score') + ggtitle("Scatter Plot of Summry Score vs Text Score")
fit = lm(apitextScore ~apisummaryScore, data = gourmetFoodsAPI)
coef(fit)
anova(fit)
summary(fit)
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"
try ({
load(url(githubURL))
})
api_key = "45319ace2f7c43fc55d05c2f973ba6261a5de4f0"
api = paste("http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=", api_key, "&outputMode=json", "&text=", sep = "")
gourmetFoodsAPI = gourmetFoods
gourmetFoodsAPI[1,]
#for i in 1:nrow(gourmetfoodsAPI){
for (i in 1:2){
if (length(gourmetFoodsAPI$text[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$text[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apitextType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apitextScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
if (length(gourmetFoodsAPI$summary[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$summary[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apisummaryType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apisummaryScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
Sys.sleep(2)
}
gourmetFoodsAPI[1,]
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"
try ({
load(url(githubURL))
})
api_key = "45319ace2f7c43fc55d05c2f973ba6261a5de4f0"
api = paste("http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=", api_key, "&outputMode=json", "&text=", sep = "")
gourmetFoodsAPI = gourmetFoods
gourmetFoodsAPI[1,]
#for i in 1:nrow(gourmetfoodsAPI){
for (i in 1:2){
if (length(gourmetFoodsAPI$text[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$text[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apitextType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apitextScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
if (length(gourmetFoodsAPI$summary[i]) > 0) {
phrase = URLencode(gourmetFoodsAPI$summary[i])
api_url = paste(api, phrase, sep="")
result = getURI(api_url)
r = fromJSON(result)
gourmetFoodsAPI$apisummaryType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
gourmetFoodsAPI$apisummaryScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
}
Sys.sleep(2)
}
gourmetFoodsAPI[1,]
