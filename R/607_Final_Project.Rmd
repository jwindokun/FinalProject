---
title: "Project Proposal Data Mash ups in R"
author: "Adejare Windokun"
date: "Friday, December 12, 2014"
output: word_document
---

### Describes your motivation for performing this analysis.
Data Mashups whereby data is obtained from different sources (heterogeneous data) and merged together to provide to a more comprehensive view of the data is of huge advantage especially with the ease of accessing data over the internet, the ability to query and use third party tools including APIs and the advent of huge datasets made available by both the government and private entities.
During the semester, while I have worked on single datasets, I haven't had the opportunity to work on multiple diverse datasets, with all the complexities that this entails - varied data formats, incomplete data, need for data transformations, and especially the need to work with third party APIs. This project will provide me the opportunity to do all these.

### Data science workflow.
My datasets consists of:

1.  A subset of the Amazon Reviews which is hosted at http://snap.stanford.edu/data/web-Amazon.html, specifically the dataset "Gourmet_Foods". This dataset is         formatted as such:

    product/productId: asin, e.g. amazon.com/dp/B00006HAXW
    product/title: title of the product
    product/price: price of the product
    review/userId: id of the user, e.g. A1RSDE90N6RSZF
    review/profileName: name of the user
    review/helpfulness: fraction of users who found the review helpful
    review/score: rating of the product
    review/time: time of the review (unix time)
    review/summary: review summary
    review/text: text of the review

where by a single row of data (item) is spread across 10 lines with a space in between subsequent items. Data transformation which has to be performed include extracting the data, limiting the dataset to manageable number, transforming the data into a table structure, cleaning the data and removing the unwanted labels, transforming columns in the dataset (covert the time column from character to POSIXct() which R understands.

2.  Reviewer data from Amazon.com using the R packages rvest and XML, which is returned in HTML format which has to be parsed to extract the relevant data which further has to be processed and converted to the required data type suitable for use in R.

3.	Sentiment Analysis using AlchemyAPI (AlchemyAPI.com). Sentiment analysis was performed on both the summary and text fields which are reviews that the reviewer made on each item. This data has to be sent to AlchemyAPI for analysis using HTML, and the result which is returned in the JSON format has to be parsed to extract the relevant data, followed by data transformation to the required data type in R for further processing.

### Project includes data from at least two different types of data sources (e.g., two or more of these: relational, CSV, Neo4J, web page, MongoDB, etc.)
My project includes (a) data from a flat file (Amazon Reviews from snap.stanford.edu), (b) live from webpages (amazon.com - http://www.amazon.com/gp/cdp/member-reviews/userID) and (c) using an API from AlchemyAPI.com

### Project includes at least one data transformation operation. [Examples: transforming from wide to long; converting columns to date format]
Each of the data sources required extensive data transformation before it could be used in R 

### Project includes at least one statistical analysis and at least one graphics that describes or validates your data.
Used linear regression in the corellation

###Project includes at least one graphic that supports your conclusion(s). 
plotted residuals vs the fitted from the linear regression

###  Project includes at least one statistical analysis that supports your conclusion(s). 
My project looks at the correlation between "Helpfullness" and "Reviewer Scores" for items and the correlation between Sentiment Analysis on "Summary Text" and
"Reviewes"

###  Project includes at least one feature that we did not cover in class! There are many examples:
Features not covered in class:
1.	AlchemyAPI (http://www.alchemyapi.com/)
2.	R packages
a.	Stringr
b.	Rvest
c.	RJSON
d.	Sqldf
e.	Rcurl


### Presentation. Did you show (at least) one challenge you encountered in code and/or data, and what you did when you encountered that challenge? If you didn't encounter any challenges, your assignment was clearly too easy for you! 

There were multiple unexpected challenges that occurred during this project.
1.	Obtaining the data. While the Amazon Review dataset is supposed to be public at http://snap.stanford.edu, one has to request permission from Stanford and this caused a delay.
2.	AlchemyAPI provides APIs to use their services, and even instructions on how these APIs work. However, they do not provide one for R, and therefore I was left to use the interface that they provide through REST. This unfortunately, was not well documented and required the manipulation of the URL, and much more manipulation of the return dataset which was in JSON format. The use of the API, for example makes submitting and extracting data a simple two line process, which however in R using the REST interface took multiple lines of code and the use of multiple R packages.
3.	Limits on data extraction using the web interface: Querying Amazon.com for reviewers other reviews was a slow process due to the fact that the query had to be sent to amazon.com using the "post" service in HTML, and the return in HTLM had to be parsed to obtain the necessary information. Also, sending the requests too fact resulted in a server denial by the amazon servers -  hence I had to build in a pause between each request.
4.	Limits on allowable use AlchemyAPI. AlchemyAPI only allows 1000 request per day from a free account . Each of my rows of data required two requests, and therefore I was limited to only being able to obtain data on 500 rows per day. In addition, too frequent requests led to a denial of service, hence I had to insert a pause between requests. Additionally, using the  REST interface led to unexpected errors (as compared to using the API) and this had to be taken into account and coded for.
5.	Inconsistent data formats. The amazon.com website does not provide a consistent format and therefore parsing the HTML return data was a slow process and had to account for this inconsistency. This led to much more coding and error handling than should have been necessary.

### Code


```{r requiredLibraries}

if(!require(stringr)) install.packages("stringr")
library(stringr)
if(!require(RCurl)) install.packages ("RCurl")
library(RCurl)
if(!require(rjson)) install.packages ("rjson")
library(rjson)
if (!require(devtools)) install.packages('devtools')
library(devtools)
if (!require(rvest)) install_github("hadley/rvest")
library(rvest)
if (!require(XML)) install.packages('XML')
library(XML)
if(!require(sqldf)) install.packages ("sqldf")
library(sqldf)

if(!require(ggplot2)) install.packages ("ggplot2")
library(ggplot2)



```


```{r amazonData}


x <- "https://raw.githubusercontent.com/jwindokun/FinalProject/master/data/gourmetFoods_test.txt"

try ({
  
line=readLines(x, n = -1L, ok = TRUE, warn = FALSE)  

})


try({
df <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
                 helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)


n = 1
d <- data.frame(productID =character(), title = character(), price = as.numeric(character()), userID =character(), profileName = character(),
                helpfulness = character(), score = as.numeric(character()), time = character(), summary = character(), text = character(),stringsAsFactors=FALSE)

}, silent = TRUE)
#for (i in 1:length(line)){
# for the purpose of demonstration will only read in the first 10000 lines of the file  
for (i in 1:1000){  
  
try({
 w = str_split_fixed(line[i], ":", 2)
    if (!is.na(w[1])){
      if (n < 11) {
        d[1, n] = str_trim(w[2])
        n = n + 1
        } else {
        df = rbind(df, d)
        n = 1
      }
    }
 
}, silent = TRUE)
  }

#clean the data

str(df)

df$time = as.POSIXct(as.numeric(df$time), origin="1970-01-01")
df$price[df$price == "unknown"] = 0
df$price = as.numeric(df$price)
df$score = as.numeric(df$score)

for (i in 1 :length(df$helpfulness)){ 
  try({

  e = df$helpfulness[i]
  e = strsplit(e, split = "/", fixed = TRUE)[[1]]
  e = as.numeric(e[1])/as.numeric(e[2])
  if (is.finite(e)) {
        df$helpfulness[i] = e
    } else {

        df$helpfulness[i] = 0
    }

}, silent = TRUE)  
}

df$helpfulness = as.numeric(df$helpfulness)
str(df)
df[1,]

```

```{r sentimentAnalysis}
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"

try ({
  
  w = load(url(githubURL))
  
})


api_key = "45319ace2f7c43fc55d05c2f973ba6261a5de4f0"


api = paste("http://access.alchemyapi.com/calls/text/TextGetTextSentiment?apikey=", api_key, "&outputMode=json", "&text=", sep = "")

gourmetFoodsAPI = gourmetFoods

gourmetFoodsAPI[1,]

#for i in 1:nrow(gourmetfoodsAPI){
for (i in 1:2){

    if (length(gourmetFoodsAPI$text[i]) > 0) {


        phrase = URLencode(gourmetFoodsAPI$text[i])
        api_url = paste(api, phrase, sep="")
        result = getURI(api_url)
        r = fromJSON(result)
        gourmetFoodsAPI$apitextType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
        gourmetFoodsAPI$apitextScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
    }


    if (length(gourmetFoodsAPI$summary[i]) > 0) {
        phrase = URLencode(gourmetFoodsAPI$summary[i])
        api_url = paste(api, phrase, sep="")
        result = getURI(api_url)
        r = fromJSON(result)
        gourmetFoodsAPI$apisummaryType[i] = ifelse(!is.null(r$docSentiment$type), r$docSentiment$type, "")
        gourmetFoodsAPI$apisummaryScore[i] = ifelse(!is.null(r$docSentiment$score), r$docSentiment$score, 0)
    }

    Sys.sleep(2)
}



gourmetFoodsAPI[1,]


```

```{r amazonWeb}
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"

try ({
  
  load(url(githubURL))
  
})


dfAmazon <- data.frame(userID = character(), reviewerName =  character(), numReviews = numeric(), itemName = character(),
                       itemPrice = numeric(), date = character(), text = character(),stringsAsFactors=FALSE)

# For demonstration purposes will only use the first 10 rows
#for (i in length(gourmetFoods)) {
for (i in 10) {

    try({
    
    userID = gourmetFoods$userID[i]
    website = paste("http://www.amazon.com/gp/cdp/member-reviews/", userID, sep ="") 
    r_site <- html(website)
    r <- r_site %>%
    html_nodes(".small") %>%
    html_text() %>%
    gsub("[\t\n\r\f\v]", "", .)


    #Get the reviewer name
    df <- data.frame(userID = character(), reviewerName =  character(), numReviews = numeric(), itemName = character(),
                     itemPrice = numeric(), date = character(), text = character(),stringsAsFactors=FALSE)


    # First line contains reviewer information
    reviewerName = substr(str_trim(r[1]), start = 13, stop = (nchar(str_trim(r[1])) -10))
   
    # Line 3 contains the total number of reviewsreviews
    numReviews = as.numeric(strsplit(str_trim(r[3]), split = " ", fixed = TRUE)[[1]][3])

   
    # get the items reviewed

    # Line 16 and (with an interval of 8) contains information on the item reviewed)
    n = 1
    for (i in seq(16,length(r),8)){
      
        df[n, "userID"] = ifelse(!is.null(userID), userID, "")
       
        df[n, "reviewerName"] = ifelse(!is.null(reviewerName), reviewerName, "")
        df[n, "numReviews"] =ifelse(!is.null(numReviews), numReviews, 0)

        t = str_split_fixed(str_trim(r[i]), ":", 2)
                
        while (nchar(t[1]) < 4){
          
          t = str_split_fixed(str_trim(r[i + 1]), ":", 2)
          i = i+1
          
        }
                      
        t = str_split_fixed(str_trim(r[i]), "Price:", 2)
        df[n, "itemName"] = ifelse(!is.null(t[1]), t[1], "")
        
        
        
        
        #df[n, "itemPrice"] =ifelse(!is.null(t[2]), as.numeric(str_sub(str_trim(t[2]),2)), 0)
        p = as.numeric(str_sub(str_trim(t[2]),2))
        f <- function(x) is.numeric(x) & !is.na(x)
        #print (f(p))
        df[n, "itemPrice"] =ifelse(f(p), p, 0)
        n = n + 1

    }

  
    # Line 20 and (with an interval of 20 contains information on the date of the review, and the text of the review)
    n = 1
    for (i in seq(20,length(r),8)){
      
        gDate <- "(([[:alpha:]]+)([[:space:]])([0-9]{1,2})([,])([[:space:]])([0-9]{4}))"
        strings <- str_trim(r[i])
        
        date = str_extract(strings, gDate)
        
        while (is.na(date)){
          
          strings <- str_trim(r[i+1])
          date = str_extract(strings, gDate)
          i = i+1
          
        }
        
        
        #date = as.Date(str_extract(strings, gDate), "%B %d, %Y")
        #print (date) 
        f = function (x) is.na(as.Date(as.character(x),format="%B/%m/%Y"))
        
        
        df[n, "date"] = ifelse(f(date), date, "")
             
          
        s = str_split_fixed(str_trim(r[i]), ":", 2)[2]
        s = str_trim(str_split_fixed(s, ")", 2)[2])
        df[n, "text"] = ifelse(!is.null(s), s, "")

        n = n + 1

    }

    dfAmazon = rbind(dfAmazon, df)
    Sys.sleep (2)

    }, silent = TRUE)

}


str(dfAmazon)
dfAmazon[1,]



```

```{r graphs, echo=FALSE}
githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoods.RData"

try ({
  
  load(url(githubURL))
  
})


t = 5

#Users with the most reviews
sql = "SELECT Count(gourmetFoods.userID) AS numReviews, gourmetFoods.profileName FROM gourmetFoods GROUP BY gourmetFoods.profileName
        HAVING (((gourmetFoods.profileName)<>'unknown')) ORDER BY Count(gourmetFoods.userID) DESC limit 10 ;"

result = sqldf(sql)


ggplot(result, aes(x = reorder(profileName, numReviews), y = numReviews)) + geom_bar(stat="identity", fill="lightblue", colour="black") +
    labs(title = "Number of Reviews per Reviewer (Top Ten Reviewers)", x = "Reviewer", y = "Count") + theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

Sys.sleep (t)

#Average Review Score by Reviewer
sql = "SELECT Count(gourmetFoods.userID) AS numReviews, gourmetFoods.profileName, Avg(gourmetFoods.score) AS AvgOfscore, Max(gourmetFoods.score) AS MaxOfscore, Min(gourmetfoods.score) AS MinOfscore
        FROM gourmetFoods GROUP BY gourmetFoods.profileName HAVING (((gourmetFoods.profileName)<>'unknown')) ORDER BY Count(gourmetFoods.userID) DESC limit 10;"

result = sqldf(sql)

ggplot(result, aes(x = reorder(profileName, AvgOfscore), y = AvgOfscore)) + geom_bar(stat="identity", fill="green", colour="red") +
    labs(title = "Average Reviewer Score By Reviewer (Top Ten Reviewers)", x = "Reviewer", y = "Average Score") + theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))


Sys.sleep (t)
#Average helpfulness per reviewer

sql = "SELECT gourmetFoods.userID, gourmetFoods.profileName, Avg(gourmetFoods.helpfulness) AS aveOfhelpfulness, Count(gourmetFoods.helpfulness) AS countOfHelpfulness
        FROM gourmetFoods GROUP BY gourmetFoods.userID, gourmetFoods.profileName ORDER BY Avg(gourmetFoods.helpfulness)  DESC , Count(gourmetFoods.helpfulness)
        DESC limit 10;"


result = sqldf(sql)

ggplot(result, aes(x = reorder(profileName, aveOfhelpfulness),  y = aveOfhelpfulness,fill = countOfHelpfulness)) + geom_bar(stat="identity", fill = "brown", colour="red") +
    labs(title = "Average Helpfulness Score By Reviewer (Top Ten Reviewers)", x = "Reviewer", y = "Average Score") + theme_bw()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))

Sys.sleep (t)
#Ploting Helpfulness score with Reviewer score

ggplot(gourmetFoods, aes(x=gourmetFoods$score,y=gourmetFoods$helpfulness)) + geom_point(position=position_jitter(w=0.1,h=0.0), color ="red") +
    geom_smooth(method="lm", se=FALSE, fill = "blue", size = 2) + ylab('Helpfulness Score') + xlab('Reviewer Score') + ggtitle("Scatter Plot of Helpfullness Score vs Reviwer Score")


# Statistics

fit = lm(helpfulness ~score, data = gourmetFoods)
coef(fit)
anova(fit)
summary(fit)

par(mfrow=c(2,2))
plot(fit)


Sys.sleep (t)
# look at Sentiment Analysis of summary and text



githubURL = "https://github.com/jwindokun/FinalProject/raw/master/data/gourmetFoodsAPI.RData"

try ({
  
  load(url(githubURL))
  
})



ggplot(gourmetFoodsAPI, aes(x=gourmetFoodsAPI$apisummaryScore,y=gourmetFoodsAPI$apitextScore)) + geom_point(position=position_jitter(w=0.1,h=0.0), color ="red") +
  geom_smooth(method="lm", se=FALSE, fill = "blue", size = 2) + ylab('Text Score') + xlab('Summary Score') + ggtitle("Scatter Plot of Summry Score vs Text Score")

fit = lm(apitextScore ~apisummaryScore, data = gourmetFoodsAPI)
coef(fit)
anova(fit)
summary(fit)
plot(fit)
par(mfrow=c(1,1))
```

